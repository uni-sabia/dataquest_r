---
title: "Predicting car prices using basic machine learning technique"
author: "Uni Lee"
date: "4/26/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(tidyverse)
library(caret)
```

Machine learning is the process of finding patterns or rules from existing data to build a model that predicts outcomes based on features of interest. In particular, the k-nearest neighbors method calculates the Euclidean distance (a metric that measures similarity) between a new data point and existing data points to find the closest neighbors to predict values of interest.

In this guided project, we will apply the k-nearest neighbors algorithm to predict car prices using the "caret" package. We will train our "machine" using Jeffrey Schlimmer's Automobile Dataset available at the [UCI Machine Learning Archive](https://archive.ics.uci.edu/ml/datasets/automobile). The data consists of information on the specification of an automobile (characteristics), insurance risk rating and its normalized losses in comparison to other cars. Insurance risk rating is denoted as "symboling", which indicates high risk with positive values and low risk with negative values (+3 = high risk, -3 = low risk). Normalized losses represents the average loss per car per year. The values take the relative average loss payment per insured vehicle year, normalized for all automobiles.   

```{r}
cars_raw <- read.table("data/17_automobile.data", sep=",")
# Rename columns as given in the data descrption https://archive.ics.uci.edu/ml/datasets/automobile 
colnames(cars_raw) <- c("symboling", "normalized_losses", "make", "fuel_type", "aspiration", "num_doors", "body_style", "drive_wheels", "engine_location", "wheel_base", "length", "width", "height", "curb_weight", "engine_type", "num_cylinders", "engine_size", "fuel_system", "bore", "stroke", "compression_ratio", "horsepower", "peak_rpm", "city_mpg", "highway_mpg", "price")

head(cars_raw)

```

The goal of this project is to predict the automobile price, which corresponds to the last column of the dataset, "price". It is a continuous variable. Since k-nearest neighbor method uses "Euclidean distance" to measure similarity between the input and existing data points, we can only use numeric variables as features used to predict the price. So, we will consider numeric variables only. 

To do that, we first have to make sure that all the columns are in the right format. We'll clean the dataset in the following steps:

* num_doors : Values are written in English. Change them to numeric values (2 or 4)
* num_cylinders : Same as above (2, 3, 4, 5, 6, 8, 12)
* normalized_losses, horsepower, peak_rpm, bore, price : Convert to integers (values denoted as "?" will automatically be converted to NAs)


```{r warning=FALSE}
cars <- cars_raw %>% mutate(
  num_doors = case_when(
    num_doors == "two" ~ 2,
    num_doors == "four" ~ 4
  ),
  num_doors = as.integer(num_doors),
  num_cylinders = case_when(
    num_cylinders == "two" ~ 2,
    num_cylinders == "three" ~ 3,
    num_cylinders == "four" ~ 4,
    num_cylinders == "five" ~ 5,
    num_cylinders == "six" ~ 6,
    num_cylinders == "eight" ~ 8,
    num_cylinders == "twelve" ~ 12),
  num_cylinders = as.integer(num_cylinders),
  normalized_losses = as.integer(normalized_losses),
  horsepower = as.integer(horsepower),
  peak_rpm = as.integer(peak_rpm),
  bore = as.integer(bore),
  price = as.integer(price)
) %>% select_if(is.numeric) %>% drop_na()

head(cars)
```

# Lattice plot: which feature is the strongest predictor of price? 

```{r}
featurePlot(x=cars, y=cars$price, plot = "scatter")
```


