---
title: "What is the right strategy to win Jeopardy?"
author: "Uni Lee"
date: "4/11/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(janitor)
library(lubridate)
library(stringr)
```

Have you ever watched Jeopardy? How about [Wer Wird Millin√§r](https://en.wikipedia.org/wiki/Wer_wird_Million%C3%A4r%3F_(German_game_show))? They are quiz shows that awards a large sum of money to people who solve trivia questions. To win the game, participants have to stufy a large database of trivia to increase chances of winning. There could be literally infinite number of trivia questions - Who has time to study them all? 

We can find an optimal winning strategy by analyzing historical data. We will identify some patterns in the questions so that we can priortize topics that appear more often. 

# Data: Jeopardy questions 1984 - 2012

Dataquest provided a subset of a large dataset of questions from the US Jeopardy show. The dataset contains 20,000 questions from episodes that aired from September 1984 until January 2012. 

```{r}
jeopardy <- read.csv("data/15_jeopardy.csv") %>% clean_names()
head(jeopardy, 5)


```

## Normalizing strings

To make our analysis easier, we will first normalize the text data. First, we will lowercase all the words and remove any punctuation. 

```{r warning=FALSE}
jeopardy_cl <- jeopardy %>% mutate(
  category = tolower(category), #To lowercase
  question = tolower(question),
  answer = tolower(question),
  value = as.numeric(substr(value, 2, 1000000L)), # Get rid of the dollar sign
  category = str_replace_all(category, "[^[:alnum:]]", " "), # Remove letters that are not alphabet or number
  question = str_replace_all(question, "[^[:alnum:]]", " "),
  answer = str_replace_all(answer, "[^[:alnum:]]", " "),
  year = year(air_date), # Separate the air date into year, month, day
  month = month(air_date),
  day = day(air_date),
  air_date=ymd(air_date)) %>%
  na.omit()


```

# Hypothesis: The probability of the question being related to science, history and Shakespeare is higher than other cateogries.

We hypothesize that questions related to science, history and Shakespeare occur more frequently than other topics. The null hypothesis is then that there is no difference between the probabilities of the topics of interest and other categories. 

## Method: Chi-squared test

We can apply Chi-squared test to see if there is a statistically significant difference between the expected and observed probabilities of getting a question related to science, history and Shakespeare. 

First, we calculate the expected probability. There are `r length(unique(jeopardy_cl$category))` number of categories in this data set. If the null hypothesis is true, the probability of picking one category is expected to be `r 1/length(unique(jeopardy_cl$category))`. The probability of not picking this particular category is then `r 1-( 1/length(unique(jeopardy_cl$category)))`. 

Second, we calculate the observed probability. The observed probabilities for a particular category can be calculated by counting the number of times that the topic appears in the Jeopardy show. As we have the observed and expected values, we can apply the Chi-squared test to see if there is a difference.

The following function performs such calculation for a given category. 

```{r}
# Parameters
n_questions <- nrow(jeopardy_cl)
n_categories <- length(unique(jeopardy_cl$category))

topics <- c("science", "history", "shakespeare")

# Expected values
expected_value <- n_questions*(1/n_categories)
## Assuming that there is no difference in probability, we expected to see the same number frequencies among the categories. 
E <- c(expected_value, expected_value, expected_value)

# Observed values   
science <- sum(str_count(jeopardy_cl$category, "science"))
history <- sum(str_count(jeopardy_cl$category, "history"))
shakespeare <- sum(str_count(jeopardy_cl$category, "shakespeare"))
O <- c(science, history, shakespeare)

# Put them into a table and calculate Chi-squared statistic
chi_table <- tibble(topics, E, O) %>%
  mutate(sqr_diff = (O-E)^2/E)

chi_stat <- sum(chi_table$sqr_diff)

# Get the p-value for this statistic
p <- 1-pchisq(chi_stat, df=2)
```

The p-value of the Chi-squared statistic is 0. We can confidently reject the null hypothesis. In other words, science, history and Shakespeare more frequently than other topics. 

# How often new questions appear? 

The Jeopardy show recycles questions. That is, questions that appeared in the past are repeated. How often do new questions appear? The following code extracts all the unique terms that have appeared in Jeopardy questions. We will use this vector to check if they have been used previously when we see a question in the future.

```{r}
# Sort in the order of ascending air date
jeopardy_cl <- jeopardy_cl %>% arrange(air_date)

# Pull unique terms that appear in the questions
unique_strings <- jeopardy_cl$question %>% unique() # This produces a string
unique_strings <- strsplit(unique_strings, " ") %>% unlist() # Split the strings by whitespace into a list and then vectorize it

# Remove words that are shorter than 7 words
unique_st <- paste(unique_strings[nchar(unique_strings)>7], collapse=" ") # This produces a string
unique_st <- strsplit(unique_st, " ") %>% unlist() # Split the strings by whitespace into a list and then vectorize it

```

# Do particular topics have higher value in Jeopardy? 

We can also optimize our winning strategy by only studying topics that have high value in the game. In the Jeopardy game, there are three low-value questions for every two high-value questions. We can set our null hypothesis to be "each category has the same share of high and low-value questions, at 2:3 ratio". The alternative hypothesis is then "some topics have different ratios for high and low-value questions". For very term that appear in the question, we will check the number of times that the question is high or low value. Using the chisq.test() function, we can calculate the p-value of the observed count of low and high value questions for each term. Note that the expected values are 2/5 for high-value and 3/5 for low-value questions. 

The terms that have the lower p value are the terms that differ from the expected values. 

```{r warning=FALSE}
# Create an empty dataset
value_data <- NULL
questions <- pull(jeopardy_cl, question)
values <- pull(jeopardy_cl, value)

for (term in unique_st[1:50]) { # We take a subset of the whole list because it's too long!
  n_high_v = 0
  n_low_v = 0

    for (i in 1:length(questions)) {
      # Split the sentence
      split = str_split(questions[i], " ")[[1]]
      
      # Check if the term is in the question and its value status
      if(term %in% split & values[i] >=800) {
        n_high_v = n_high_v + 1
      } else if (term %in% split & values[i] < 800) {
        n_low_v = n_low_v + 1
      }
    }
  
  # Chi-squared test
  test = chisq.test(c(n_high_v, n_low_v), p = c(2/5, 3/5))
  new_row = c(term, n_high_v, n_low_v, test$p.value)
  
  # Add (append) this to a new row in the empty dataset.
  value_data <- rbind(value_data, new_row)
  colnames(value_data) <- c("term", "n_high_v", "n_low_v", "p_value")
  value_data <- as.data.frame(value_data) %>% distinct()
} 
```

```{r}
value_data <- value_data %>% 
  filter(n_high_v > n_low_v) %>% # only the ones that have higher ratio of high value questions
  arrange(p_value)
head(value_data, 10)
```


# Conclusion

Our data showed that questions related to science, history and Shakespeare appear more often than other topics, rejecting the null hypothesis. Participants can benefit from focusing on these topics. They can further optimize their study strategy by focusing on questions related to terms that are associated higher proportion of high-value questions. 

